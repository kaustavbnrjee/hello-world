#!/usr/bin/env python
# coding: utf-8

# In[3]:



get_ipython().system('pip install ftfy')
get_ipython().system('pip install sklearn')


# In[1]:


# -*- coding: utf-8 -*-
"""
Created on Fri Oct 18 11:19:25 2024

@author: tranm
"""

### Exercise 1 ###


import nltk

import sklearn

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from nltk.tokenize import word_tokenize
from collections import Counter
import os, pandas as pd, emoji, re, numpy as np

pd.set_option('mode.chained_assignment',None)
pd.set_option('display.max_columns',50)
pd.set_option('display.width',200)

datafolder = "/Users/mohammadhosseinrashidi/Dropbox/BUSADM 797-01/Data"
stpath = f'{datafolder}/GMEstocktwitsV2_sample.csv.gz'

stdf = pd.read_csv(stpath)

sample = stdf.sample(1000,random_state=7)
# get sample tweet:  #not important, just a test
test = sample['text'].tolist()[0]
Counter(word_tokenize(test))


#Create a new column wich is a bag of words
#sample['bow'] = sample['text'].apply(lambda x: Counter(word_tokenize(str(x))) if x is not None else Counter())





#dtm1 = pd.DataFrame(sample['bow'].values.tolist()).fillna(0)
# dtm1 not very good, not the best way to generate


# Replace np.nan with an empty string and convert all entries to string
sample['text'] = sample['text'].fillna('').astype(str)



vec = CountVectorizer()  # Initializes a word count vectorizer
dtm2 = vec.fit_transform(sample['text']) #Transforms text data into a sparse document-term matrix.




dtm2.todense() # generates numpy matrix
#np.asarray(dtm2.todense()) # generates numpy array


example = pd.DataFrame(dtm2.todense(),
             columns=vec.get_feature_names_out(),
             index=sample.index)     #Creates a DataFrame of word counts with proper labels for columns (words) and rows (documents).

#.todense() generates a matrix object, which behaves differently from
# a regular ndarray. If you need the flexibility of working with a 
#general ndarray (which supports more operations and has better 
#compatibility with most NumPy functions), you convert it using np.asarray()



dtm2.sum(axis=1) # Total word counts per document.
dtm2.sum(axis=0) # Total counts of each word across all documents.






example = pd.DataFrame(dtm2.todense(),
             columns=vec.get_feature_names_out(),
             index=sample.index)     #Creates a DataFrame of word counts with proper labels for columns (words) and rows (documents).


example.sum(axis=1) # Total word counts per document.
example.sum(axis=0)

#axis=0 means summing across rows
#axis=1 means summing across columns 


###ANOTHER WAY TO DO THIS

import ftfy

#The ftfy library is used here to fix encoding errors and clean up
# text data. Specifically, ftfy.ftfy is a function that attempts 
#to "fix" common problems with text, such as garbled characters, 
#invalid Unicode sequences, or other encoding issues.


#a = ftfy.ftfy('ÂªÃ°')


sample['text'] = sample['text'].apply(ftfy.ftfy)   # Cleans text data by fixing encoding issues using the ftfy library




dtm2 = vec.fit_transform(sample['text'])  #Converts the cleaned text into a document-term matrix of word counts using CountVectorizer



dtm2_df = pd.DataFrame(dtm2.todense(),
             columns=vec.get_feature_names_out(),
             index=sample.index)

#Converts the sparse matrix of word counts into a dense matrix and 
#creates a DataFrame, labeling the columns with the words (features)
#and keeping the original document indices.


dtm2_df.sum(axis=0).sort_values(ascending=False)   # Total counts of each word across all documents.


dtm2_df.sum(axis=1).sort_values(ascending=False)  # Total word counts per document.



#create another dataset without stopwords

from nltk.corpus import stopwords
stops = stopwords.words('english')   
vec = CountVectorizer(stop_words=stops,min_df=2)     

#This argument passes the stops list (which contains the English stopwords) to the CountVectorizer. As a result, any word in the stopwords list will be ignored or filtered out when the CountVectorizer processes the text

#  min_df=2: only words that appear in at least 2 documents will be considered. Words that appear in fewer than 2 documents will be excluded from the document-term matrix.

dtm2b = vec.fit_transform(sample['text'])
dtm2b_df = pd.DataFrame(dtm2b.todense(),
             columns=vec.get_feature_names_out(),
             index=sample.index)


dtm2b_df.sum(axis=0).sort_values(ascending=False) 

dtm2b_df.sum(axis=1).sort_values(ascending=False)   # Total counts of each word across all documents.






# -*- coding: utf-8 -*-
"""
Created on Fri Oct 18 11:19:25 2024

@author: tranm
"""

### Exercise 1 ###


import nltk

import sklearn

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from nltk.tokenize import word_tokenize
from collections import Counter
import os, pandas as pd, emoji, re, numpy as np

pd.set_option('mode.chained_assignment',None)
pd.set_option('display.max_columns',50)
pd.set_option('display.width',200)

datafolder = "/Users/tranm/Dropbox/UMASS Class/BUSADM 797/Data"
stpath = f'{datafolder}/GMEstocktwitsV2_sample.csv.gz'

stdf = pd.read_csv(stpath)

sample = stdf.sample(1000,random_state=7)
# get sample tweet:  #not important, just a test
test = sample['text'].tolist()[0]
Counter(word_tokenize(test))


#Create a new column wich is a bag of words
#sample['bow'] = sample['text'].apply(lambda x: Counter(word_tokenize(str(x))) if x is not None else Counter())





#dtm1 = pd.DataFrame(sample['bow'].values.tolist()).fillna(0)
# dtm1 not very good, not the best way to generate


# Replace np.nan with an empty string and convert all entries to string
sample['text'] = sample['text'].fillna('').astype(str)



vec = CountVectorizer()  # Initializes a word count vectorizer
dtm2 = vec.fit_transform(sample['text']) #Transforms text data into a sparse document-term matrix.




dtm2.todense() # generates numpy matrix
#np.asarray(dtm2.todense()) # generates numpy array


example = pd.DataFrame(dtm2.todense(),
             columns=vec.get_feature_names_out(),
             index=sample.index)     #Creates a DataFrame of word counts with proper labels for columns (words) and rows (documents).

#.todense() generates a matrix object, which behaves differently from
# a regular ndarray. If you need the flexibility of working with a 
#general ndarray (which supports more operations and has better 
#compatibility with most NumPy functions), you convert it using np.asarray()



dtm2.sum(axis=1) # Total word counts per document.
dtm2.sum(axis=0) # Total counts of each word across all documents.






example = pd.DataFrame(dtm2.todense(),
             columns=vec.get_feature_names_out(),
             index=sample.index)     #Creates a DataFrame of word counts with proper labels for columns (words) and rows (documents).


example.sum(axis=1) # Total word counts per document.
example.sum(axis=0)

#axis=0 means summing across rows
#axis=1 means summing across columns 


###ANOTHER WAY TO DO THIS

import ftfy

#The ftfy library is used here to fix encoding errors and clean up
# text data. Specifically, ftfy.ftfy is a function that attempts 
#to "fix" common problems with text, such as garbled characters, 
#invalid Unicode sequences, or other encoding issues.


#a = ftfy.ftfy('ÂªÃ°')


sample['text'] = sample['text'].apply(ftfy.ftfy)   # Cleans text data by fixing encoding issues using the ftfy library




dtm2 = vec.fit_transform(sample['text'])  #Converts the cleaned text into a document-term matrix of word counts using CountVectorizer



dtm2_df = pd.DataFrame(dtm2.todense(),
             columns=vec.get_feature_names_out(),
             index=sample.index)

#Converts the sparse matrix of word counts into a dense matrix and 
#creates a DataFrame, labeling the columns with the words (features)
#and keeping the original document indices.


dtm2_df.sum(axis=0).sort_values(ascending=False)   # Total counts of each word across all documents.


dtm2_df.sum(axis=1).sort_values(ascending=False)  # Total word counts per document.



#create another dataset without stopwords

from nltk.corpus import stopwords
stops = stopwords.words('english')   
vec = CountVectorizer(stop_words=stops,min_df=2)     

#This argument passes the stops list (which contains the English stopwords) to the CountVectorizer. As a result, any word in the stopwords list will be ignored or filtered out when the CountVectorizer processes the text

#  min_df=2: only words that appear in at least 2 documents will be considered. Words that appear in fewer than 2 documents will be excluded from the document-term matrix.

dtm2b = vec.fit_transform(sample['text'])
dtm2b_df = pd.DataFrame(dtm2b.todense(),
             columns=vec.get_feature_names_out(),
             index=sample.index)


dtm2b_df.sum(axis=0).sort_values(ascending=False) 

dtm2b_df.sum(axis=1).sort_values(ascending=False)   # Total counts of each word across all documents.







# In[ ]:


# -*- coding: utf-8 -*-
# EN: Encoding declaration for UTF-8 to support non-English characters
# FA: ØªØ¹Ø±ÛŒÙ Ù†ÙˆØ¹ Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ÛŒ ØºÛŒØ± Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ

"""
Created on Fri Oct 18 11:19:25 2024
@author: tranm
"""
# EN: Metadata about the file and author
# FA: Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ØªØ§ Ø¯Ø± Ù…ÙˆØ±Ø¯ ÙØ§ÛŒÙ„ Ùˆ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡

### Exercise 1 ###
# EN: Marks the beginning of Exercise 1
# FA: Ø´Ø±ÙˆØ¹ ØªÙ…Ø±ÛŒÙ† Ø´Ù…Ø§Ø±Ù‡ Û±

import nltk
# EN: Natural Language Toolkit library for tokenizing, stopwords, etc.
# FA: Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ø´Ø§Ù…Ù„ ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ú©Ù„Ù…Ø§Øª ØªÙˆÙ‚Ù

import sklearn
# EN: Machine learning library, used here for text vectorization
# FA: Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ú©Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¹Ø¯Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
# EN: Tools to transform text into count or tf-idf matrix
# FA: Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ù…Ø§ØªØ±ÛŒØ³ ÙØ±Ú©Ø§Ù†Ø³ ÛŒØ§ TF-IDF

from nltk.tokenize import word_tokenize
# EN: Function to split sentences into words
# FA: ØªÙˆØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª

from collections import Counter
# EN: For counting frequency of each word
# FA: Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ ÙˆÙ‚ÙˆØ¹ Ù‡Ø± Ú©Ù„Ù…Ù‡

import os, pandas as pd, emoji, re, numpy as np
# EN: Load standard libraries for file ops, dataframes, regex, arrays, emoji handling
# FA: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø± Ø¨Ø§ ÙØ§ÛŒÙ„ØŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø¸Ù… Ùˆ Ø§ÛŒÙ…ÙˆØ¬ÛŒ

pd.set_option('mode.chained_assignment',None)
# EN: Suppresses SettingWithCopyWarning in Pandas
# FA: Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§Ù†ØªØ³Ø§Ø¨ Ø²Ù†Ø¬ÛŒØ±Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ù¾Ø§Ù†Ø¯Ø§Ø³

pd.set_option('display.max_columns',50)
# EN: Show up to 50 columns in dataframe output
# FA: Ù†Ù…Ø§ÛŒØ´ Ø­Ø¯Ø§Ú©Ø«Ø± ÛµÛ° Ø³ØªÙˆÙ† Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø§Ø¯Ù‡

pd.set_option('display.width',200)
# EN: Extend console width for better DataFrame visibility
# FA: ØªÙ†Ø¸ÛŒÙ… Ø¹Ø±Ø¶ Ù†Ù…Ø§ÛŒØ´ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ØªØ± Ø¯ÛŒØ¯Ù‡ Ø´Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§

datafolder = "/Users/mohammadhosseinrashidi/Dropbox/BUSADM 797-01/Data"
# EN: Path to the directory containing data
# FA: Ù…Ø³ÛŒØ± ÙÙˆÙ„Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§

stpath = f'{datafolder}/GMEstocktwitsV2_sample.csv.gz'
# EN: Full path to the StockTwits dataset file (compressed CSV)
# FA: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ÙØ´Ø±Ø¯Ù‡ Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÛŒÛŒØªØ± Ø¯Ø± Ù…ÙˆØ±Ø¯ GME

stdf = pd.read_csv(stpath)
# EN: Read CSV file into a DataFrame
# FA: Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ CSV Ø¨Ù‡ ØµÙˆØ±Øª Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…

sample = stdf.sample(1000,random_state=7)
# EN: Randomly sample 1000 tweets for analysis
# FA: Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ ØªØµØ§Ø¯ÙÛŒ Ø§Ø² Û±Û°Û°Û° ØªÙˆÛŒÛŒØª Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„

test = sample['text'].tolist()[0]
# EN: Select first tweet from sample for testing
# FA: Ø§Ù†ØªØ®Ø§Ø¨ Ø§ÙˆÙ„ÛŒÙ† ØªÙˆÛŒÛŒØª Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´

Counter(word_tokenize(test))
# EN: Tokenize the tweet and count each word's frequency
# FA: ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ ØªÙˆÛŒÛŒØª Ùˆ Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø± Ù‡Ø± Ú©Ù„Ù…Ù‡

# sample['bow'] = sample['text'].apply(lambda x: Counter(word_tokenize(str(x))) if x is not None else Counter())
# EN: (Commented) Attempt to build bag-of-words column by word frequency per tweet
# FA: (ØºÛŒØ±ÙØ¹Ø§Ù„) Ø³Ø§Ø®Øª Ø³ØªÙˆÙ† Ú©ÛŒØ³Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨Ø±Ø§ÛŒ Ù‡Ø± ØªÙˆÛŒÛŒØª

# dtm1 = pd.DataFrame(sample['bow'].values.tolist()).fillna(0)
# EN: (Commented) Transform bag-of-words dictionary into DataFrame
# FA: (ØºÛŒØ±ÙØ¹Ø§Ù„) ØªØ¨Ø¯ÛŒÙ„ ÙˆØ§Ú˜Ù‡â€ŒÙ†Ø§Ù…Ù‡ Ø¨Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…

sample['text'] = sample['text'].fillna('').astype(str)
# EN: Replace missing texts with empty strings and convert all to string type
# FA: Ù¾Ø± Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ù‡Ù…Ù‡ Ø¨Ù‡ Ø±Ø´ØªÙ‡

vec = CountVectorizer()
# EN: Initialize CountVectorizer for creating DTM
# FA: Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ CountVectorizer Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ù…Ø§ØªØ±ÛŒØ³ Ø³Ù†Ø¯-ÙˆØ§Ú˜Ù‡

dtm2 = vec.fit_transform(sample['text'])
# EN: Fit and transform text data into a sparse DTM
# FA: Ø§Ø¹Ù…Ø§Ù„ CountVectorizer Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…Ø§ØªØ±ÛŒØ³ Ø³Ù†Ø¯-ÙˆØ§Ú˜Ù‡

dtm2.todense()
# EN: Convert sparse matrix to dense matrix (for better visualization)
# FA: ØªØ¨Ø¯ÛŒÙ„ Ù…Ø§ØªØ±ÛŒØ³ ØªÙ†Ú© Ø¨Ù‡ Ù…Ø§ØªØ±ÛŒØ³ Ù…ØªØ±Ø§Ú©Ù…

example = pd.DataFrame(dtm2.todense(),
             columns=vec.get_feature_names_out(),
             index=sample.index)
# EN: Create labeled DataFrame of word counts with words as columns
# FA: Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø§ ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø³ØªÙˆÙ†

dtm2.sum(axis=1)
# EN: Total word count per document (tweet)
# FA: ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ù‡Ø± ØªÙˆÛŒÛŒØª

dtm2.sum(axis=0)
# EN: Total frequency of each word across all documents
# FA: ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ØªÚ©Ø±Ø§Ø± Ù‡Ø± ÙˆØ§Ú˜Ù‡ Ø¯Ø± ØªÙ…Ø§Ù… ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§

example.sum(axis=1)
# EN: Confirm row sums (word counts per tweet)
# FA: Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¬Ù…ÙˆØ¹ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ (ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ù‡Ø± ØªÙˆÛŒÛŒØª)

example.sum(axis=0)
# EN: Confirm column sums (frequency of each word)
# FA: Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¬Ù…ÙˆØ¹ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ (ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ÛŒÚ© ÙˆØ§Ú˜Ù‡)

# axis=0 means summing across rows
# axis=1 means summing across columns
# FA: axis=0 ÛŒØ¹Ù†ÛŒ Ø¬Ù…Ø¹ Ø²Ø¯Ù† Ø¯Ø± Ø¨ÛŒÙ† Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ Ùˆ axis=1 ÛŒØ¹Ù†ÛŒ Ø¬Ù…Ø¹ Ø¨ÛŒÙ† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§

import ftfy
# EN: Fix Text for You â€“ a library to repair Unicode and text encoding issues
# FA: Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ù…Ø´Ú©Ù„Ø§Øª Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§

sample['text'] = sample['text'].apply(ftfy.ftfy)
# EN: Apply text fixing to clean encoding issues in tweets
# FA: Ø§ØµÙ„Ø§Ø­ Ù…Ø´Ú©Ù„Ø§Øª Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù…ØªÙ† ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§

dtm2 = vec.fit_transform(sample['text'])
# EN: Re-vectorize cleaned text into document-term matrix
# FA: Ø§Ø¹Ù…Ø§Ù„ CountVectorizer Ø±ÙˆÛŒ Ù…ØªÙ† ØªÙ…ÛŒØ² Ø´Ø¯Ù‡

dtm2_df = pd.DataFrame(dtm2.todense(),
             columns=vec.get_feature_names_out(),
             index=sample.index)
# EN: Convert matrix to DataFrame for word count inspection
# FA: ØªØ¨Ø¯ÛŒÙ„ Ù…Ø§ØªØ±ÛŒØ³ Ø¨Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª

dtm2_df.sum(axis=0).sort_values(ascending=False)
# EN: Sort and view most frequent words in corpus
# FA: Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù¾Ø±ØªÚ©Ø±Ø§Ø±ØªØ±ÛŒÙ† ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§

dtm2_df.sum(axis=1).sort_values(ascending=False)
# EN: View tweets with highest word counts
# FA: Ù…Ø´Ø§Ù‡Ø¯Ù‡ ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ù‡

from nltk.corpus import stopwords
# EN: Import standard list of English stopwords
# FA: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª ØªÙˆÙ‚Ù Ø²Ø¨Ø§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ

stops = stopwords.words('english')
# EN: Load stopwords into a list
# FA: ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª ØªÙˆÙ‚Ù Ø¨Ù‡ ÛŒÚ© Ù„ÛŒØ³Øª

vec = CountVectorizer(stop_words=stops,min_df=2)
# EN: Reinitialize CountVectorizer to ignore stopwords and exclude rare words
# FA: Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø± Ùˆ Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª Ø®ÛŒÙ„ÛŒ Ú©Ù…â€ŒØªÚ©Ø±Ø§Ø± (Ú©Ù…ØªØ± Ø§Ø² Û² Ø³Ù†Ø¯)

dtm2b = vec.fit_transform(sample['text'])
# EN: Fit and transform text again using new filtering rules
# FA: ØªÙˆÙ„ÛŒØ¯ Ù…Ø§ØªØ±ÛŒØ³ Ø³Ù†Ø¯-ÙˆØ§Ú˜Ù‡ Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¬Ø¯ÛŒØ¯

dtm2b_df = pd.DataFrame(dtm2b.todense(),
             columns=vec.get_feature_names_out(),
             index=sample.index)
# EN: Convert matrix to labeled DataFrame
# FA: ØªØ¨Ø¯ÛŒÙ„ Ù…Ø§ØªØ±ÛŒØ³ Ø¨Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø§ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ú˜Ù‡

dtm2b_df.sum(axis=0).sort_values(ascending=False)
# EN: Sort most common non-stopwords across tweets
# FA: Ù†Ù…Ø§ÛŒØ´ Ù¾Ø±ØªÚ©Ø±Ø§Ø±ØªØ±ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø¨Ø¯ÙˆÙ† Ú©Ù„Ù…Ø§Øª ØªÙˆÙ‚Ù

dtm2b_df.sum(axis=1).sort_values(ascending=False)
# EN: Show tweet lengths based on filtered vocabulary
# FA: Ù†Ù…Ø§ÛŒØ´ Ø·ÙˆÙ„ ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ± Ø´Ø¯Ù† ÙˆØ§Ú˜Ú¯Ø§Ù†


# In[3]:


ğŸ”¹ 1. Remove Emojis or Special Characters
get_ipython().set_next_input('ğŸ§ª Question: How can you clean emojis or special Unicode characters from the tweets');get_ipython().run_line_magic('pinfo', 'tweets')

ğŸ’¡ Why? Emojis are non-standard symbols that can mislead word counts and DTM representations.

âœ… Solution:

python
Copy
Edit
import re
sample['text'] = sample['text'].apply(lambda x: re.sub(emoji.get_emoji_regexp(), '', x))


ğŸ”¹ 2. Use TF-IDF Instead of Raw Counts
get_ipython().set_next_input('ğŸ§ª Question: Can we use TF-IDF instead of word frequency');get_ipython().run_line_magic('pinfo', 'frequency')

ğŸ’¡ Why? TF-IDF normalizes term importance and reduces the weight of common terms.

âœ… Solution:

python
Copy
Edit
vec = TfidfVectorizer()
dtm_tfidf = vec.fit_transform(sample['text'])


ğŸ”¹ 3. Remove Very Short Words (e.g., < 3 characters)
get_ipython().set_next_input('ğŸ§ª Question: How do you remove short meaningless words');get_ipython().run_line_magic('pinfo', 'words')

ğŸ’¡ Why? Short words like "an", "to", "go" are often semantically weak.

âœ… Solution:

python
Copy
Edit
vec = CountVectorizer(token_pattern=r'(?u)\b\w\w\w+\b')  # Words with 3 or more letters



ğŸ”¹ 4. Filter Rare Words (min_df)
get_ipython().set_next_input('ğŸ§ª Question: How do you ignore words that appear in only 1 or 2 tweets');get_ipython().run_line_magic('pinfo', 'tweets')

ğŸ’¡ Why? Rare words add noise and reduce model generalization.

âœ… Solution:

python
Copy
Edit
vec = CountVectorizer(min_df=2)



ğŸ”¹ 5. Filter Frequent Words (max_df)
get_ipython().set_next_input('ğŸ§ª Question: What if I want to ignore overly frequent words that appear in almost all documents');get_ipython().run_line_magic('pinfo', 'documents')

ğŸ’¡ Why? Extremely common words may not add value for classification.

âœ… Solution:

python
Copy
Edit
vec = CountVectorizer(max_df=0.90)  # Ignore words in more than 90% of tweets



ğŸ”¹ 6. Compare DTM With and Without Stopwords
get_ipython().set_next_input('ğŸ§ª Question: Whatâ€™s the difference in feature count with and without stopwords');get_ipython().run_line_magic('pinfo', 'stopwords')

âœ… Solution:

python
Copy
Edit
len(CountVectorizer().fit(sample['text']).get_feature_names_out())
len(CountVectorizer(stop_words='english').fit(sample['text']).get_feature_names_out())



ğŸ”¹ 7. Use Pre-tokenized Text (Avoid Double Tokenizing)
get_ipython().set_next_input('ğŸ§ª Question: What if the text is already tokenized');get_ipython().run_line_magic('pinfo', 'tokenized')

ğŸ’¡ Why? Applying CountVectorizer again would break it.

âœ… Solution:

python
Copy
Edit
vec = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x)  # Pass-through
vec.fit_transform(sample['tokenized_column'])  # tokenized_column = list of tokens


ğŸ”¹ 8. Get Word Frequency Across All Tweets
get_ipython().set_next_input('ğŸ§ª Question: How do you find the most common words overall');get_ipython().run_line_magic('pinfo', 'overall')

âœ… Solution:

python
Copy
Edit
dtm_df.sum(axis=0).sort_values(ascending=False).head(10)


ğŸ”¹ 9. Get Longest Tweet (by word count)
get_ipython().set_next_input('ğŸ§ª Question: How do you find which tweet is the longest');get_ipython().run_line_magic('pinfo', 'longest')

âœ… Solution:

python
Copy
Edit
dtm_df.sum(axis=1).sort_values(ascending=False).head(1)


ğŸ”¹ 10. Visualize Word Frequencies
get_ipython().set_next_input('ğŸ§ª Question: Can you show the top 20 most frequent words in a bar chart');get_ipython().run_line_magic('pinfo', 'chart')

âœ… Solution:

python
Copy
Edit
import matplotlib.pyplot as plt
dtm_df.sum(axis=0).sort_values(ascending=False).head(20).plot(kind='bar')
plt.title("Top 20 Words")
plt.show()
ğŸ”¹ 11. Fix Corrupt Characters
get_ipython().set_next_input('ğŸ§ª Question: How do you fix corrupted characters in text');get_ipython().run_line_magic('pinfo', 'text')

âœ… Solution:

python
Copy
Edit
import ftfy
sample['text'] = sample['text'].apply(ftfy.fix_text)


ğŸ”¹ 12. Normalize Case (Lowercase All Words)
get_ipython().set_next_input('ğŸ§ª Question: How do you ensure â€˜Gameâ€™ and â€˜gameâ€™ are treated the same');get_ipython().run_line_magic('pinfo', 'same')

âœ… Solution:

python
Copy
Edit
sample['text'] = sample['text'].str.lower()



ğŸ”¹ 13. Remove Numbers or Tokens with Digits
ğŸ§ª Question: How can you remove numeric tokens like â€˜gme2023â€™?

âœ… Solution:

python
Copy
Edit
vec = CountVectorizer(token_pattern=r'(?u)\b[a-zA-Z]{3,}\b')



ğŸ”¹ 14. Extract N-grams (e.g., bigrams)
ğŸ§ª Question: Can you get phrase-level tokens like "short squeeze"?

âœ… Solution:

python
Copy
Edit
vec = CountVectorizer(ngram_range=(2,2))  # For bigrams only



ğŸ”¹ 15. Get Vocabulary Size
get_ipython().set_next_input('ğŸ§ª Question: How many unique words are in your vectorizer');get_ipython().run_line_magic('pinfo', 'vectorizer')

âœ… Solution:

python
Copy
Edit
len(vec.get_feature_names_out())



ğŸ”¹ 16. Export DTM for ML Use
get_ipython().set_next_input('ğŸ§ª Question: How would you export the document-term matrix for modeling');get_ipython().run_line_magic('pinfo', 'modeling')

âœ… Solution:

python
Copy
Edit
dtm_df.to_csv('dtm_output.csv')

