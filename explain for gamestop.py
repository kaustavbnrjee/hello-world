#!/usr/bin/env python
# coding: utf-8

# In[3]:


#############################################
####MEASURE SENTIMENT OF ONLINE ARTICLE######
#############################################



import pandas as pd, numpy as np
pd.set_option('display.max_columns',50)
pd.set_option('display.width',200)
pd.set_option('mode.chained_assignment',None)

datafolder =  "/Users/mohammadhosseinrashidi/Dropbox/BUSADM 797-01/Data"


# Type 4: Seeking Alpha Articles (measure sentiment)
from bs4 import BeautifulSoup
import os
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter
import nltk
nltk.download('punkt_tab')

##############
###GME sample is the Seeking Alpha's sentiment of GME stock around 2018-2019 when the pumping happened


sapath = f"{datafolder}/GMESample"
safiles = [f for f in os.listdir(sapath) if f.endswith('htm')]

# from os import scandir
# for f in scandir(sapath):
#     print(f.name)
#     print(f.path)
#     break

lmpath = f'{datafolder}/LoughranMcDonald_SentimentWordLists_2018.xlsx'     ###take the positive and negative word from the dictionary

####Next: extract the positive and negative word from the dictionary

negatives = set(pd.read_excel(lmpath,sheet_name='Negative',
                              names=['word'],header=None)['word'].str.lower())



positives = set(pd.read_excel(lmpath,sheet_name='Positive',
                              names=['word'],header=None)['word'].str.lower())

stopwords = set([w.lower() for w in stopwords.words('english')])







#####take a sample from the link to test the function later #########


###This line opens the file located at the path f'{sapath}/{art}' (constructed by concatenating the sapath directory and the file name art) in read mode ('r').

art = '4161066.htm'
with open(f'{sapath}/{art}','r', encoding='utf-8', errors='ignore') as f:
    contents = f.read()
    

#parsing the document, represents the HTML document in a structured format, allowing you to easily navigate and manipulate the documentâ€™s elements 

soup = BeautifulSoup(contents,'lxml')

# from each article collect title, firm, timestamp, author, article
record = dict()
# title
soup.find('h1',attrs={'data-test-id':'post-title'})




    
    
    
# ticker
ticker = soup.find('span',attrs={'data-test-id':'post-primary-tickers'})
if ticker:
    record['ticker'] = ticker.get_text()
    
    
    
# timestamp
timestamp = soup.find('span',attrs={'data-test-id':'post-date'})
if timestamp:
    record['timestamp'] = timestamp.get_text()
    
    
# author
author = soup.find('a',attrs={'data-test-id':'author-name'})
if author:
    record['author'] = author.get_text()
    
    
    
# article contents
article = soup.find('div',attrs={'data-test-id':'article-content'})
if article:
    text = article.get_text()
    


#Tokenization is the process of splitting a sequence of text into smaller pieces, such as words or phrases
words = word_tokenize(text)   #list all the words and its length

#count frequency of each words in the article
wordcounts = Counter([w.lower() for w in words if w.isalpha() and w not in stopwords])













# set up our sentiment variables
record['totalwords'] = sum(wordcounts.values())
record['poswords'] = sum([v for k,v in wordcounts.items() if k in positives])
record['negwords'] = sum([v for k,v in wordcounts.items() if k in negatives])


#This code defines a function parse_article(art) that parses an article 
#from an HTML file and extracts key information like title, ticker,
# timestamp, author, and the article's content.





#The total number of words, positive words, and negative words is 
#computed by comparing each word in the text to predefined lists of 
#positive (positives) and negative (negatives) words.



def parse_article(art):
    with open(f'{sapath}/{art}', 'r',encoding='utf-8', errors='ignore') as f:
        contents = f.read()
    # Create the soup
    soup = BeautifulSoup(contents, 'lxml')
    # set up dictionary to collect info
    record = dict()
    # title
    title = soup.find('h1', attrs={'data-test-id': 'post-title'})
    if title:
        record['title'] = title.get_text().strip()
    # ticker
    ticker = soup.find('span', attrs={'data-test-id': 'post-primary-tickers'})
    if ticker:
        record['ticker'] = ticker.get_text()
    # timestamp
    timestamp = soup.find('span', attrs={'data-test-id': 'post-date'})
    if timestamp:
        record['timestamp'] = timestamp.get_text()
    # author
    author = soup.find('a', attrs={'data-test-id': 'author-name'})
    if author:
        record['author'] = author.get_text()
    # article contents
    article = soup.find('div', attrs={'data-test-id': 'article-content'})
    text = ""
    if article:
        text = article.get_text()
        
    

    if len(text)>0:
        # Compute sentiment
        words = word_tokenize(text)
        wordcounts = Counter([w.lower() for w in words if w.isalpha() and w not in stopwords])
        # set up our sentiment variables
        record['totalwords'] = sum(wordcounts.values())
        record['poswords'] = sum([v for k, v in wordcounts.items() if k in positives])
        record['negwords'] = sum([v for k, v in wordcounts.items() if k in negatives])

    return record


#w.isalpha(): only keep alphabetic words
#w.lower() covert all words to lower for consistency
#stopwrods: e.g. the, is,  and are ignored
#summing the counts (v) of words (k) in wordcounts that are present in the positives






#Enumerate to record all of the GameStop articles

allsa = []
for i,art in enumerate(safiles):
    allsa.append(parse_article(art))
    print(i)
    
    



sadf = pd.DataFrame(allsa)

#Simple calulation of sentiment 
sadf['sentiment'] = sadf.eval('(poswords-negwords)/totalwords')


# In[ ]:


# Load essential libraries for data handling
import pandas as pd, numpy as np  # Data analysis libraries
# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡

# Configure pandas to display more columns and avoid warnings
pd.set_option('display.max_columns',50)  # Show up to 50 columns
pd.set_option('display.width',200)       # Set terminal width
pd.set_option('mode.chained_assignment',None)  # Suppress SettingWithCopyWarning
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ØªØ± Ø¯ÛŒØ¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø®Ø·Ø§Ø±Ù‡Ø§ÛŒ ØºÛŒØ± Ø¶Ø±ÙˆØ±ÛŒ

# Set the data folder path
datafolder =  "/Users/mohammadhosseinrashidi/Dropbox/BUSADM 797-01/Data"
# Ù…Ø³ÛŒØ± ÙÙˆÙ„Ø¯Ø± Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…

# Load sentiment analysis libraries
from bs4 import BeautifulSoup  # For parsing HTML
import os  # To interact with the operating system (file reading)
from nltk.tokenize import word_tokenize  # For splitting text into words
from nltk.corpus import stopwords  # Common English stop words
from collections import Counter  # Count frequency of words
import nltk
nltk.download('punkt_tab')  # Ensure tokenization resources are downloaded
# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ ØªØ¬Ø²ÛŒÙ‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ† Ùˆ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ HTML

# Define the path for Seeking Alpha articles related to GME
sapath = f"{datafolder}/GMESample"
safiles = [f for f in os.listdir(sapath) if f.endswith('htm')]  # Filter HTML files
# Ù…Ø³ÛŒØ± Ù…Ù‚Ø§Ù„Ø§Øª Ø³Ø§ÛŒØª Seeking Alpha Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø³Ù‡Ø§Ù… GME Ø¯Ø± Ø³Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ

# Define path for Loughran and McDonald sentiment dictionary
lmpath = f'{datafolder}/LoughranMcDonald_SentimentWordLists_2018.xlsx' 
# Ù…Ø³ÛŒØ± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†ÙÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ

# Load negative words
negatives = set(pd.read_excel(lmpath,sheet_name='Negative',
                              names=['word'],header=None)['word'].str.lower())
# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ù…Ù†ÙÛŒ Ø§Ø² ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ (set)

# Load positive words
positives = set(pd.read_excel(lmpath,sheet_name='Positive',
                              names=['word'],header=None)['word'].str.lower())
# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ù…Ø«Ø¨Øª Ø§Ø² ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ (set)

# Get standard English stopwords (like "the", "is", etc.)
stopwords = set([w.lower() for w in stopwords.words('english')])
# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ú©ÙˆÚ†Ú©â€ŒÙ†ÙˆÛŒØ³ÛŒ Ú©Ù„Ù…Ø§Øª Ø±Ø§ÛŒØ¬ Ø¨ÛŒâ€ŒØ§Ù‡Ù…ÛŒØª Ø¯Ø± Ø²Ø¨Ø§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ

# Choose a sample article for testing
art = '4161066.htm'

# Read HTML content from the file
with open(f'{sapath}/{art}','r', encoding='utf-8', errors='ignore') as f:
    contents = f.read()
# Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ø­ØªÙˆØ§ÛŒ ÛŒÚ© Ù…Ù‚Ø§Ù„Ù‡ Ù†Ù…ÙˆÙ†Ù‡

# Parse HTML using BeautifulSoup
soup = BeautifulSoup(contents,'lxml')
# ØªØ¬Ø²ÛŒÙ‡ ÙØ§ÛŒÙ„ HTML Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ØµØ± Ù…Ù‡Ù…

record = dict()  # Dictionary to store parsed info

# Extract ticker
ticker = soup.find('span',attrs={'data-test-id':'post-primary-tickers'})
if ticker:
    record['ticker'] = ticker.get_text()
# Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÛŒÚ©Ø± Ø´Ø±Ú©Øª (Ù…Ø«Ù„Ø§Ù‹ GME)

# Extract timestamp
timestamp = soup.find('span',attrs={'data-test-id':'post-date'})
if timestamp:
    record['timestamp'] = timestamp.get_text()
# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ´Ø§Ø± Ù…Ù‚Ø§Ù„Ù‡

# Extract author
author = soup.find('a',attrs={'data-test-id':'author-name'})
if author:
    record['author'] = author.get_text()
# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ù…Ù‚Ø§Ù„Ù‡

# Extract full article content
article = soup.find('div',attrs={'data-test-id':'article-content'})
if article:
    text = article.get_text()
# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§Ù…Ù„ Ù…Ù‚Ø§Ù„Ù‡

# Tokenize words and count word frequency (ignore punctuation and stopwords)
words = word_tokenize(text)
wordcounts = Counter([w.lower() for w in words if w.isalpha() and w not in stopwords])
# ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†ØŒ Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ Ùˆ Ú©Ù„Ù…Ø§Øª Ø±Ø§ÛŒØ¬ØŒ Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙØ¹Ø§Øª Ù‡Ø± Ú©Ù„Ù…Ù‡

# Compute sentiment metrics
record['totalwords'] = sum(wordcounts.values())
record['poswords'] = sum([v for k,v in wordcounts.items() if k in positives])
record['negwords'] = sum([v for k,v in wordcounts.items() if k in negatives])
# Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ú©Ù„Ù…Ø§ØªØŒ Ú©Ù„Ù…Ø§Øª Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†ÙÛŒ

# Define the main function to extract and process an article
def parse_article(art):
    with open(f'{sapath}/{art}', 'r',encoding='utf-8', errors='ignore') as f:
        contents = f.read()
    soup = BeautifulSoup(contents, 'lxml')
    record = dict()

    title = soup.find('h1', attrs={'data-test-id': 'post-title'})
    if title:
        record['title'] = title.get_text().strip()

    ticker = soup.find('span', attrs={'data-test-id': 'post-primary-tickers'})
    if ticker:
        record['ticker'] = ticker.get_text()

    timestamp = soup.find('span', attrs={'data-test-id': 'post-date'})
    if timestamp:
        record['timestamp'] = timestamp.get_text()

    author = soup.find('a', attrs={'data-test-id': 'author-name'})
    if author:
        record['author'] = author.get_text()

    article = soup.find('div', attrs={'data-test-id': 'article-content'})
    text = ""
    if article:
        text = article.get_text()

    if len(text)>0:
        words = word_tokenize(text)
        wordcounts = Counter([w.lower() for w in words if w.isalpha() and w not in stopwords])
        record['totalwords'] = sum(wordcounts.values())
        record['poswords'] = sum([v for k, v in wordcounts.items() if k in positives])
        record['negwords'] = sum([v for k, v in wordcounts.items() if k in negatives])

    return record
# ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ Ø§Ø² ÛŒÚ© Ù…Ù‚Ø§Ù„Ù‡ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ù†Ø¬Ø´ Ø§Ø­Ø³Ø§Ø³

# Loop through all Seeking Alpha articles and parse them
allsa = []
for i,art in enumerate(safiles):
    allsa.append(parse_article(art))
    print(i)
# Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ø±ÙˆÛŒ ØªÙ…Ø§Ù… Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø­Ø³Ø§Ø³ Ú©Ù„ÛŒ

# Store results in a DataFrame
sadf = pd.DataFrame(allsa)
# ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ³Øª Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…

# Calculate sentiment score
sadf['sentiment'] = sadf.eval('(poswords-negwords)/totalwords')
# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…Ø±Ù‡ Ø§Ø­Ø³Ø§Ø³ Ø¨Ø§ ÙØ±Ù…ÙˆÙ„ Ø³Ø§Ø¯Ù‡: Ù…Ø«Ø¨Øª Ù…Ù†ÙÛŒ ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± Ú©Ù„ Ú©Ù„Ù…Ø§Øª


# # ðŸŸ¦ Scenario 1: Add Neutral Words
# Request: â€œI want to include neutral words in the analysis.â€
# 
# What to change:
# 
# Add a new sheet to the Loughran-McDonald Excel for neutral words.
# 
# Load them just like positives and negatives:
# 
# #### start code:
# neutrals = set(pd.read_excel(lmpath, sheet_name='Neutral',
#                              names=['word'], header=None)['word'].str.lower())
# ### continueAdd this to parse_article to calculate neutral word count:
# 
# 
# record['neutralwords'] = sum([v for k, v in wordcounts.items() if k in neutrals])
# ### finish
# ÙØ§Ø±Ø³ÛŒ: Ø¨Ø±Ø§ÛŒ Ù„Ø­Ø§Ø¸ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ø®Ù†Ø«ÛŒØŒ ÛŒÚ© Sheet Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù† Ùˆ Ù…Ø´Ø§Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†ÙÛŒ Ø¢Ù† Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†.
# 
# 

# # ðŸŸ¦ Scenario 2: Filter Articles by Date Range
# Request: â€œOnly analyze articles from Jan 2019 to Dec 2019.â€
# 
# What to do:
# 
# Inside the parse_article loop, convert timestamp to datetime:
# 
# ### start
# record['timestamp'] = pd.to_datetime(record['timestamp'])
# ### continue Filter sadf after creation:
# 
# 
# sadf = sadf[(sadf['timestamp'] >= '2019-01-01') & (sadf['timestamp'] <= '2019-12-31')]
# ### finish
# ÙØ§Ø±Ø³ÛŒ: ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ® Ø¨Ù‡ datetime Ùˆ Ø³Ù¾Ø³ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† ÙÙ‚Ø· Ù…Ù‚Ø§Ù„Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ø¨Ø§Ø²Ù‡ Û²Û°Û±Û¹ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯.
# 
# 

# # Scenario 3: Add Sentiment Categories (Positive/Neutral/Negative)
# Request: â€œLabel each article as â€˜Positiveâ€™, â€˜Neutralâ€™, or â€˜Negativeâ€™.â€
# 
# What to add:
# 
# ### start
# def label_sentiment(score):
#     if score > 0.01:
#         return 'Positive'
#     elif score < -0.01:
#         return 'Negative'
#     else:
#         return 'Neutral'
# 
# sadf['sentiment_label'] = sadf['sentiment'].apply(label_sentiment)
# ### finish
# ÙØ§Ø±Ø³ÛŒ: ÛŒÚ© ØªØ§Ø¨Ø¹ ØªØ¹Ø±ÛŒÙ Ú©Ù† Ú©Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² Ø§Ø­Ø³Ø§Ø³ØŒ Ø¨Ø±Ú†Ø³Ø¨ Ù…Ø«Ø¨ØªØŒ Ù…Ù†ÙÛŒ ÛŒØ§ Ø®Ù†Ø«ÛŒ Ø¨Ø¯Ù‡Ø¯.

# # Scenario 4: Use More Advanced Sentiment Models (e.g., VADER)
# Request: â€œCan we use a machine learning-based or pretrained model like VADER?â€
# 
# What to do:
# 
# Add from nltk.sentiment.vader import SentimentIntensityAnalyzer
# 
# Load analyzer: sia = SentimentIntensityAnalyzer()
# 
# In parse_article, replace simple counts:
# 
# ### start
# sentiment_scores = sia.polarity_scores(text)
# record['compound_score'] = sentiment_scores['compound']
# ### fnish
# ÙØ§Ø±Ø³ÛŒ: Ø¨Ù‡ Ø¬Ø§ÛŒ Ø´Ù…Ø§Ø±Ø´ Ø¯Ø³ØªÛŒ Ú©Ù„Ù…Ø§Øª Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†ÙÛŒØŒ Ø§Ø² Ù…Ø¯Ù„ Ø¢Ù…Ø§Ø¯Ù‡ NLTK Ù…Ø«Ù„ VADER Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† Ú©Ù‡ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø§Ø±Ù‡.
# 
# 

# # 5. Track Article Length as a Feature
# Question: â€œHow would you measure the average article length and assess its relationship with sentiment?â€
# 
# Answer:
# 
# Add in parse_article():
# 
# ### start
# record['article_length'] = len(text.split())  # Word count of article
# ### Reason:
# 
# Helps test if longer articles tend to be more positive/negative.
# 
# Enables regression or visualization.
# 
# 6. Normalize Sentiment by Paragraph or Sentence
# Question: â€œWhat if longer articles distort sentiment scores?â€
# 
# Answer:
# 
# Use average sentiment per sentence:
# 
# ### start
# 
# from nltk.tokenize import sent_tokenize
# nltk.download('punkt')  # Required for sentence splitting
# record['avg_sentiment'] = sentiment_scores['compound'] / len(sent_tokenize(text))
# ### Reason:
# 
# Normalization prevents longer articles from biasing results.
# 
# 7. Store Raw Article Text for Later Use
# Question: â€œHow can we reuse text for LLM analysis or vector embedding?â€
# 
# Answer:
# 
# Add to record:
# 
# ### start
# 
# record['raw_text'] = text
# ### Reason:
# 
# Needed for text classification, GPT summarization, etc.
# 
# 8. Handle Missing or Corrupt Articles
# Question: â€œSome files are broken or empty. Whatâ€™s your solution?â€
# 
# Answer:
# 
# Wrap code in tryâ€“except:
# 
# ### start
# 
# try:
#     with open(...) as f:
#         ...
# except Exception as e:
#     print(f"Skipping {art}: {e}")
#     return None
# In the loop:
# 
# ### start
# 
# parsed = parse_article(art)
# if parsed:
#     allsa.append(parsed)
# ### Reason:
# 
# Makes pipeline robust and production-ready.
# 
# 9. Compare LM Sentiment vs. VADER
# Question: â€œHow can you test if LM and VADER agree?â€
# 
# Answer:
# 
# Calculate correlation:
# 
# ### start
# 
# sadf[['sentiment', 'compound_score']].corr()
# ### Reason:
# 
# Validate consistency across different methods.
# 
# 10. Export Results to Excel/CSV
# Question: â€œHow can you save sentiment scores for future use?â€
# 
# Answer:
# 
# ### start
# 
# sadf.to_csv('gme_sentiment.csv', index=False)
# ### Reason:
# 
# Needed for sharing, modeling, dashboarding.
# 
# 11. Add Ticker Filtering
# Question: â€œWhat if I only want articles about a specific ticker, like 'GME'?â€
# 
# Answer:
# 
# ### start
# 
# sadf = sadf[sadf['ticker'] == 'GME']
# ### Reason:
# 
# Allows comparison across firms or sectors.
# 
# 12. Time Series Analysis of Sentiment
# Question: â€œCan we track how sentiment changed over time?â€
# 
# Answer:
# 
# ### start
# 
# sadf.set_index('timestamp').resample('M')['sentiment'].mean().plot()
# ### Reason:
# 
# Useful for matching sentiment to stock price or events.
# 
# 13. Detect Sarcasm or Complex Tone
# Advanced Bonus Question:
# â€œCan this pipeline detect sarcasm?â€
# 
# Answer:
# 
# No, rule-based methods like LM or VADER donâ€™t handle sarcasm.
# 
# Need fine-tuned transformer models (e.g., BERT, GPT).
# 
# 14. Visualize Top Positive/Negative Words
# Question: â€œCan you visualize the most common sentiment words?â€
# 
# Answer:
# 
# ### start
# 
# from wordcloud import WordCloud
# WordCloud().generate_from_frequencies({k:v for k,v in wordcounts.items() if k in positives}).to_image()
